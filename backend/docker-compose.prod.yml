services:
  mqtt:
    image: eclipse-mosquitto:2
    ports:
      - "1883:1883"
    volumes:
      - ./services/mqtt-bridge/mosquitto.prod.conf:/mosquitto/config/mosquitto.conf:ro
      - ./services/mqtt-bridge/passwords:/mosquitto/config/passwords:ro
      - ./services/mqtt-bridge/acl:/mosquitto/config/acl:ro
      - mqtt_data:/mosquitto/data
      - mqtt_logs:/mosquitto/log
    restart: unless-stopped
  db:
    image: timescale/timescaledb:latest-pg16
    environment:
      # ВАЖНО: Замените на уникальный пароль через переменную окружения
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}
      POSTGRES_USER: ${POSTGRES_USER:-hydro}
      POSTGRES_DB: ${POSTGRES_DB:-hydro}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - wal_archive:/wal_archive
    command:
      - "postgres"
      - "-c"
      - "archive_mode=on"
      - "-c"
      - "wal_level=replica"
      - "-c"
      - "archive_command='test ! -f /wal_archive/%f && cp %p /wal_archive/%f'"
      # Performance optimizations
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "effective_cache_size=1GB"
      - "-c"
      - "maintenance_work_mem=128MB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "work_mem=4MB"
      - "-c"
      - "min_wal_size=1GB"
      - "-c"
      - "max_wal_size=4GB"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "max_worker_processes=8"
      - "-c"
      - "max_parallel_workers_per_gather=4"
      - "-c"
      - "max_parallel_workers=8"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    restart: unless-stopped
  redis:
    image: redis:7-alpine
    command: >
      redis-server
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save ""
      --appendonly no
      --tcp-backlog 511
      --timeout 0
      --tcp-keepalive 300
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    restart: unless-stopped
  laravel:
    build:
      context: ./laravel
      dockerfile: Dockerfile
      args:
        INSTALL_DEV_DEPS: "false"
        VITE_REVERB_HOST: localhost
        VITE_WS_HOST: localhost
        VITE_REVERB_PORT: "6001"
        VITE_WS_PORT: "6001"
        VITE_REVERB_SCHEME: http
        VITE_WS_TLS: "false"
        VITE_ENABLE_WS: "true"
        VITE_REVERB_APP_KEY: ${REVERB_APP_KEY:-change-me}
        VITE_PUSHER_APP_KEY: ${REVERB_APP_KEY:-change-me}
    working_dir: /app
    # Reverb запускается автоматически через supervisor (reverb-supervisor.conf)
    # Не запускаем через nohup, чтобы избежать двойного запуска и ошибки EADDRINUSE
    # command: оставлен по умолчанию (docker-entrypoint.sh запустит supervisord)
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    environment:
      - WEB_DOCUMENT_ROOT=/app/public
      - APP_ENV=production
      - APP_DEBUG=false
      - SESSION_DRIVER=redis
      - CACHE_DRIVER=redis
      - LOG_CHANNEL=stderr
      - LOG_LEVEL=info
      - BROADCAST_CONNECTION=reverb
      - QUEUE_CONNECTION=redis
      # PHP-FPM performance settings
      - PHP_FPM_PM=dynamic
      - PHP_FPM_PM_MAX_CHILDREN=50
      - PHP_FPM_PM_START_SERVERS=10
      - PHP_FPM_PM_MIN_SPARE_SERVERS=5
      - PHP_FPM_PM_MAX_SPARE_SERVERS=20
      - PHP_FPM_PM_MAX_REQUESTS=500
      - PHP_FPM_PM_PROCESS_IDLE_TIMEOUT=10s
      # Nginx worker processes (auto-detected, but can be set)
      - NGINX_WORKER_PROCESSES=auto
      - NGINX_WORKER_CONNECTIONS=1024
      - DB_CONNECTION=pgsql
      - DB_HOST=db
      - DB_PORT=5432
      - DB_DATABASE=hydro
      - DB_USERNAME=hydro
      - DB_PASSWORD=${POSTGRES_PASSWORD:-change-me}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REVERB_APP_ID=${REVERB_APP_ID:-app}
      # ВАЖНО: Замените на уникальные ключи через переменные окружения
      - REVERB_APP_KEY=${REVERB_APP_KEY:?REVERB_APP_KEY must be set}
      - REVERB_APP_SECRET=${REVERB_APP_SECRET:?REVERB_APP_SECRET must be set}
      - REVERB_HOST=0.0.0.0
      - REVERB_PORT=6001
      - REVERB_SCHEME=http
      - REVERB_DEBUG=false
      - REVERB_AUTO_START=${REVERB_AUTO_START:-true}
      # WebSocket security: явный список разрешенных origins
      - REVERB_ALLOWED_ORIGINS=${REVERB_ALLOWED_ORIGINS:?REVERB_ALLOWED_ORIGINS must be set}
      # Кросс-доменная авторизация для фронта на отдельном домене
      # SANCTUM_STATEFUL_DOMAINS: домены, с которых разрешены cookie-based запросы (через запятую)
      # SESSION_DOMAIN: домен для сессионных cookie (например .example.com для всех поддоменов)
      - SANCTUM_STATEFUL_DOMAINS=${SANCTUM_STATEFUL_DOMAINS:?SANCTUM_STATEFUL_DOMAINS must be set}
      - SESSION_DOMAIN=${SESSION_DOMAIN:-}
      - SESSION_SECURE_COOKIE=${SESSION_SECURE_COOKIE:-true}
      # Vite environment variables for WebSocket (передаются при сборке)
      # ВАЖНО: Для браузера используем localhost, а не 0.0.0.0
      - VITE_ENABLE_WS=true
      - VITE_REVERB_APP_KEY=${REVERB_APP_KEY:-change-me}
      - VITE_REVERB_HOST=localhost
      - VITE_REVERB_PORT=6001
      - VITE_REVERB_SCHEME=http
      - VITE_PUSHER_APP_KEY=${REVERB_APP_KEY:-change-me}
      - VITE_WS_HOST=localhost
      - VITE_WS_PORT=6001
      - VITE_WS_TLS=false
      # ВАЖНО: Токены для безопасности сервисов
      - PY_API_TOKEN=${PY_API_TOKEN:?PY_API_TOKEN must be set}
      - PY_INGEST_TOKEN=${PY_INGEST_TOKEN:?PY_INGEST_TOKEN must be set}
      - NODE_SIM_MANAGER_URL=${NODE_SIM_MANAGER_URL:-http://node-sim-manager:9100}
      - NODE_SIM_MANAGER_TOKEN=${NODE_SIM_MANAGER_TOKEN:-}
    volumes:
      - ./laravel:/app
    ports:
      - "8080:80"
      - "6001:6001"
    depends_on:
      - db
      - redis
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/api/system/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
  # api-gateway: роль выполняется Laravel (см. backend/laravel/)
  mqtt-bridge:
    build:
      context: ./services
      dockerfile: mqtt-bridge/Dockerfile
    environment:
      - APP_ENV=production
      - MQTT_HOST=mqtt
      - MQTT_PORT=1883
      - MQTT_TLS=0
      - MQTT_USER=${MQTT_MQTT_BRIDGE_USER:-mqtt_bridge}
      # ВАЖНО: Замените на уникальный пароль через переменную окружения
      - MQTT_PASS=${MQTT_MQTT_BRIDGE_PASS:?MQTT_MQTT_BRIDGE_PASS must be set}
      - PG_HOST=db
      - PG_DB=hydro
      - PG_USER=hydro
      - PG_PASS=${POSTGRES_PASSWORD:-change-me}
      - LARAVEL_API_URL=http://laravel
      # ВАЖНО: Токены для безопасности сервисов
      - PY_API_TOKEN=${PY_API_TOKEN:?PY_API_TOKEN must be set}
      - LARAVEL_API_TOKEN=${LARAVEL_API_TOKEN:?LARAVEL_API_TOKEN must be set}
    depends_on:
      - mqtt
      - db
      - laravel
    ports:
      - "9000:9000"
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"localhost\", 9000)); s.close()' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
  # device-registry: PLANNED - функционал частично в Laravel (см. backend/services/device-registry/README.md)
  # device-registry:
  #   build: ./services/device-registry
  #   depends_on: [db]
  automation-engine:
    build:
      context: ./services
      dockerfile: automation-engine/Dockerfile
    environment:
      - APP_ENV=production
      - MQTT_HOST=mqtt
      - MQTT_PORT=1883
      - MQTT_TLS=0
      - MQTT_USER=${MQTT_AUTOMATION_ENGINE_USER:-automation_engine}
      # ВАЖНО: Замените на уникальный пароль через переменную окружения
      - MQTT_PASS=${MQTT_AUTOMATION_ENGINE_PASS:?MQTT_AUTOMATION_ENGINE_PASS must be set}
      - PG_HOST=db
      - PG_DB=hydro
      - PG_USER=hydro
      - PG_PASS=${POSTGRES_PASSWORD:-change-me}
      - LARAVEL_API_URL=http://laravel
      # ВАЖНО: Токены для безопасности сервисов
      - PY_API_TOKEN=${PY_API_TOKEN:?PY_API_TOKEN must be set}
      - LARAVEL_API_TOKEN=${LARAVEL_API_TOKEN:?LARAVEL_API_TOKEN must be set}
    depends_on:
      - mqtt
      - db
      - laravel
    ports:
      - "9401:9401"
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"localhost\", 9401)); s.close()' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
  history-logger:
    build:
      context: ./services
      dockerfile: history-logger/Dockerfile
    environment:
      - APP_ENV=production
      - MQTT_HOST=mqtt
      - MQTT_PORT=1883
      - MQTT_TLS=0
      - MQTT_USER=${MQTT_HISTORY_LOGGER_USER:-history_logger}
      # ВАЖНО: Замените на уникальный пароль через переменную окружения
      - MQTT_PASS=${MQTT_HISTORY_LOGGER_PASS:?MQTT_HISTORY_LOGGER_PASS must be set}
      - PG_HOST=db
      - PG_DB=hydro
      - PG_USER=hydro
      - PG_PASS=${POSTGRES_PASSWORD:-change-me}
      - LARAVEL_API_URL=http://laravel
      # ВАЖНО: Токены для безопасности сервисов
      - PY_API_TOKEN=${PY_API_TOKEN:?PY_API_TOKEN must be set}
      - PY_INGEST_TOKEN=${PY_INGEST_TOKEN:?PY_INGEST_TOKEN must be set}
      - HISTORY_LOGGER_API_TOKEN=${HISTORY_LOGGER_API_TOKEN:-${PY_INGEST_TOKEN:?HISTORY_LOGGER_API_TOKEN or PY_INGEST_TOKEN must be set}}
      - LARAVEL_API_TOKEN=${LARAVEL_API_TOKEN:?LARAVEL_API_TOKEN must be set}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - mqtt
      - db
      - laravel
      - redis
    ports:
      - "9300:9300"
      - "9301:9301"  # Prometheus metrics
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"localhost\", 9300)); s.close()' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./configs/prod/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./configs/prod/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    depends_on:
      - automation-engine
      - scheduler
      - mqtt-bridge
    restart: unless-stopped
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      # ВАЖНО: Замените на уникальный пароль через переменную окружения
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD must be set}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      - GF_DEFAULT_LANGUAGE=ru
      - GF_USERS_DEFAULT_LANGUAGE=ru
    volumes:
      - ./configs/prod/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      - ./configs/prod/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./configs/prod/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
      - db
    restart: unless-stopped
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - ./configs/prod/alertmanager/config.yml:/etc/alertmanager/config.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - "9093:9093"
    depends_on:
      - prometheus
      - laravel
    restart: unless-stopped
  scheduler:
    build:
      context: ./services
      dockerfile: scheduler/Dockerfile
    environment:
      - APP_ENV=production
      - MQTT_HOST=mqtt
      - MQTT_PORT=1883
      - MQTT_TLS=0
      - MQTT_USER=${MQTT_SCHEDULER_USER:-scheduler}
      # ВАЖНО: Замените на уникальный пароль через переменную окружения
      - MQTT_PASS=${MQTT_SCHEDULER_PASS:?MQTT_SCHEDULER_PASS must be set}
      - PG_HOST=db
      - PG_DB=hydro
      - PG_USER=hydro
      - PG_PASS=${POSTGRES_PASSWORD:-change-me}
    depends_on:
      - mqtt
      - db
    ports:
      - "9402:9402"
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"localhost\", 9402)); s.close()' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
  digital-twin:
    build:
      context: ./services
      dockerfile: digital-twin/Dockerfile
    environment:
      - APP_ENV=production
      - PG_HOST=db
      - PG_PORT=5432
      - PG_DB=hydro
      - PG_USER=hydro
      - PG_PASS=${POSTGRES_PASSWORD:-change-me}
      - PYTHONPATH=/app
    depends_on:
      - db
    ports:
      - "8003:8003"
      - "9403:9403"  # Prometheus metrics
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"localhost\", 8003)); s.close()' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
  node-sim-manager:
    build:
      context: ./services
      dockerfile: node-sim-manager/Dockerfile
    environment:
      - APP_ENV=production
      - LOG_LEVEL=INFO
      - NODE_SIM_ROOT=/node-sim
    volumes:
      - ../tests/node_sim:/node-sim:ro
    depends_on:
      - mqtt
    ports:
      - "9100:9100"
    restart: unless-stopped
  telemetry-aggregator:
    build:
      context: ./services
      dockerfile: telemetry-aggregator/Dockerfile
    environment:
      - APP_ENV=production
      - PG_HOST=db
      - PG_PORT=5432
      - PG_DB=hydro
      - PG_USER=hydro
      - PG_PASS=${POSTGRES_PASSWORD:-change-me}
      - PYTHONPATH=/app
    depends_on:
      - db
    ports:
      - "9404:9404"  # Prometheus metrics
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"localhost\", 9404)); s.close()' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped

volumes:
  postgres_data:
  prometheus_data:
  mqtt_data:
  mqtt_logs:
  grafana_data:
  alertmanager_data:
  wal_archive:
