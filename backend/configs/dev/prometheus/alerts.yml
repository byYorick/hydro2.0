groups:
  - name: node_alerts
    interval: 30s
    rules:
      - alert: NodeOffline
        expr: |
          (time() - node_last_seen_timestamp{status="offline"}) > 300
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.uid }} is offline"
          description: "Node {{ $labels.uid }} ({{ $labels.name }}) has been offline for more than 5 minutes"

  - name: service_alerts
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up{job=~"automation-engine|scheduler|mqtt-bridge|history-logger"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} has been down for more than 2 minutes"

      - alert: MQTTBrokerDown
        expr: up{job="mqtt-bridge"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MQTT broker is unreachable"
          description: "MQTT bridge service is not responding"

  - name: zone_alerts
    interval: 30s
    rules:
      - alert: ZoneCriticalAlert
        expr: |
          increase(zone_critical_alerts_total[5m]) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Zone {{ $labels.zone_id }} has multiple critical alerts"
          description: "Zone {{ $labels.zone_id }} has generated more than 3 critical alerts in the last 5 minutes"

      - alert: HighCommandFailureRate
        expr: |
          rate(commands_failed_total[5m]) / rate(commands_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High command failure rate"
          description: "Command failure rate is above 10% for the last 5 minutes"

  - name: history_logger_alerts
    interval: 30s
    rules:
      - alert: HistoryLoggerQueueOverflow
        expr: telemetry_queue_size > 10000
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "History Logger queue is overflowing"
          description: "Telemetry queue size is {{ $value }} (threshold: 10000). Messages may be dropped."

      - alert: HistoryLoggerQueueStale
        expr: telemetry_queue_age_seconds > 300
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "History Logger queue has stale items"
          description: "Oldest item in queue is {{ $value }} seconds old (threshold: 300s). Processing may be delayed."

      - alert: HistoryLoggerDroppingMessages
        expr: rate(telemetry_dropped_total{reason="queue_push_failed"}[5m]) > 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "History Logger is dropping messages"
          description: "Dropping {{ $value }} messages/sec due to queue push failures. Data loss is occurring."

      - alert: HistoryLoggerDatabaseErrors
        expr: rate(database_errors_total[5m]) > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "History Logger has database errors"
          description: "Database error rate is {{ $value }} errors/sec. Check database connectivity and performance."

      - alert: HistoryLoggerSlowProcessing
        expr: histogram_quantile(0.99, rate(telemetry_processing_duration_seconds_bucket[5m])) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "History Logger processing is slow"
          description: "P99 processing time is {{ $value }}s (threshold: 5s). Performance degradation detected."

      - alert: HistoryLoggerNoProcessing
        expr: rate(telemetry_processed_total[5m]) == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "History Logger is not processing telemetry"
          description: "No telemetry messages processed in the last 5 minutes. Service may be stuck."

  - name: automation_engine_alerts
    interval: 30s
    rules:
      - alert: AutomationEngineLoopErrors
        expr: rate(automation_loop_errors_total[5m]) > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Automation Engine has loop errors"
          description: "Error rate in main loop is {{ $value }} errors/sec. Check service logs for details."

      - alert: AutomationEngineConfigFetchErrors
        expr: rate(config_fetch_errors_total[5m]) > 3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Automation Engine cannot fetch configuration"
          description: "Config fetch error rate is {{ $value }} errors/sec. Check Laravel API connectivity."

      - alert: AutomationEngineMQTTPublishErrors
        expr: rate(mqtt_publish_errors_total[5m]) > 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Automation Engine cannot publish commands"
          description: "MQTT publish error rate is {{ $value }} errors/sec. Commands are not being sent to nodes."

      - alert: AutomationEngineSlowZoneProcessing
        expr: histogram_quantile(0.99, rate(zone_check_seconds_bucket[5m])) > 30
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Automation Engine zone processing is slow"
          description: "P99 zone check duration is {{ $value }}s (threshold: 30s). Performance degradation detected."

      - alert: AutomationEngineNoZoneChecks
        expr: rate(zone_checks_total[5m]) == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Automation Engine is not processing zones"
          description: "No zone checks performed in the last 5 minutes. Service may be stuck or configuration is invalid."

      - alert: AutomationEngineHighErrorRate
        expr: |
          (sum(rate(automation_loop_errors_total[5m])) + 
           sum(rate(automation_errors_total[5m])) + 
           sum(rate(mqtt_publish_errors_total[5m]))) > 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Automation Engine has high error rate"
          description: "Total error rate is {{ $value }} errors/sec. Multiple components may be failing."

  - name: scheduler_task_alerts
    interval: 30s
    rules:
      - alert: SchedulerTaskAcceptLatencyHigh
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (rate(scheduler_task_accept_latency_sec_bucket[5m]))
          ) > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Scheduler task accept latency is high"
          description: "P95 accept latency is {{ $value }}s (threshold: 2s). Scheduler -> automation-engine handoff is degraded."

      - alert: SchedulerTaskCompletionLatencyHigh
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (rate(scheduler_task_completion_latency_sec_bucket{status="completed"}[5m]))
          ) > 60
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Scheduler task completion latency is high"
          description: "P95 completion latency is {{ $value }}s (threshold: 60s). Automation execution is slower than expected."

      - alert: SchedulerTaskFailureRateHigh
        expr: |
          sum(
            rate(scheduler_task_status_total{status=~"failed|timeout|submit_failed|not_found"}[5m])
          ) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Scheduler task failure rate is high"
          description: "Failure rate is {{ $value }} tasks/sec for terminal error statuses."

      - alert: SchedulerActiveTasksBacklogHigh
        expr: scheduler_active_tasks > 100
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Scheduler active tasks backlog is high"
          description: "Active scheduler tasks are {{ $value }} (threshold: 100). Possible stuck reconciliation or automation slowdown."

  - name: scheduler_automation_slo_alerts
    interval: 30s
    rules:
      - alert: SchedulerTaskAcceptToTerminalLatencySLODegraded
        expr: |
          histogram_quantile(
            0.95,
            sum by (le) (rate(task_accept_to_terminal_latency_bucket[5m]))
          ) > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Task accept->terminal latency SLO degraded"
          description: "P95 task_accept_to_terminal_latency is {{ $value }}s (threshold: 90s)."

      - alert: SchedulerTaskDeadlineViolationRateHigh
        expr: task_deadline_violation_rate > 0.05
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Scheduler deadline violation rate is high"
          description: "task_deadline_violation_rate is {{ $value }} (threshold: 0.05)."

      - alert: AutomationCommandEffectConfirmRateLow
        expr: command_effect_confirm_rate < 0.95
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Command effect confirm rate is low"
          description: "command_effect_confirm_rate is {{ $value }} (threshold: 0.95). Closed-loop DONE confirmations are degrading."

      - alert: AutomationTaskRecoverySuccessRateLow
        expr: task_recovery_success_rate < 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Task recovery success rate is low"
          description: "task_recovery_success_rate is {{ $value }} (threshold: 0.9). Check restart recovery behavior."
